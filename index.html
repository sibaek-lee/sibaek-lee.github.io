<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Sibaek Lee </title> <meta name="author" content="Sibaek Lee"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="sibaek lee, 이시백, ai researcher, robotics researcher, artificial intelligence, machine learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/prof_pic.jpg?1ff95527c052f585dcebdf338fca46b0"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sibaek-lee.github.io/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="d-flex align-items-center"> <div class="navbar-brand social me-2" style="font-size: 1.5em; margin-bottom: 0;"> <a href="mailto:%6C%6D%6A%62%73%6A%31@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/sibaek-lee" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/sibaek-lee-008b1a2a1" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://scholar.google.com/citations?user=BG206AgAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://youtube.com/@aigsmusic" title="YouTube" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> <a href="https://www.youtube.com/@genesisbeats8070" title="Composition" rel="external nofollow noopener" target="_blank"> <img src="https://twemoji.maxcdn.com/v/latest/72x72/1f3b9.png" alt="Composition"> </a> </div> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Sibaek Lee </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic.jpg?1ff95527c052f585dcebdf338fca46b0" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <style>.profile img{max-width:70%!important}h2{margin-top:3rem!important;margin-bottom:1rem!important}</style> </div> </div> <div class="clearfix"> <p>Hi!😀 I am a PhD student in <a href="https://robot.skku.edu/robot_en/index.do" rel="external nofollow noopener" target="_blank">Intelligent Robotics</a> at Sungkyunkwan University advised by Prof. <a href="https://bogus2000.github.io/" rel="external nofollow noopener" target="_blank">Hyeonwoo yu</a> where I am a member of SKKU Artificial Intelligence and Robotics (<a href="https://sites.google.com/view/hyeonwooyu/" rel="external nofollow noopener" target="_blank">Lair</a>).</p> <p>As a researcher, I hope my research contributes positively to society and I believe that artificial intelligence is a step towards achieving that goal. My research focuses on using deep learning techniques to enhance robots’ spatial perception. By improving their ability to perceive their environment accurately, we can enable robots to make better decisions, perform more complex tasks, and ultimately help people in various ways. <br> <br></p> <h4 id="research-interests">Research Interests</h4> <ul> <li> <strong>🧠 Deep Learning:</strong> 3D Vision, Vision Language Model, Generative model, Bayesian learning</li> <li> <strong>🤖 Robotics:</strong> Simultaneous Localization and Mapping (SLAM), Robot Perception and navigation</li> </ul> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Sep 2025</th> <td> 🎉 Our paper “LAMP: Implicit Language Map for Robot Navigation” is accepted to RA-L 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 2025</th> <td> 🏆 Honored to receive the LG AI Research Best Paper Award at the Conference on Korean Artificial Intelligence Association (CKAIA) 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 2025</th> <td> 🎉 Our paper “Spatial Coordinate Transformation for 3D Neural Implicit Mapping” is accepted to RA-L 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 2025</th> <td> 🎉 Our paper “Bayesian NeRF: Quantifying Uncertainty With Volume Density for Neural Implicit Fields” is accepted to RA-L 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 2024</th> <td> 🏢 Joined <a href="https://www.naverlabs.com/" rel="external nofollow noopener" target="_blank">Naver LABS</a> (Vision Group) as a 3D Vision &amp; Deep Learning Research Intern. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 2024</th> <td> 🎉 Our paper “Just flip: Flipped observation generation and optimization for neural radiance fields to cover unobserved view” is accepted to IROS 2024. </td> </tr> </table> </div> </div> <h2 style="margin-bottom: 0.5rem;"> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <style>.badge.badge-kw-1{background-color:#7dd3fc!important;color:#0c4a6e!important}.badge.badge-kw-2{background-color:#94a3b8!important;color:#1e293b!important}.badge.badge-kw-3{background-color:#6ee7b7!important;color:#064e3b!important}.badge.badge-kw-4{background-color:#fbbf24!important;color:#451a03!important}.badge.badge-kw-5{background-color:#fb7185!important;color:white!important}.badge.badge-kw-6{background-color:#818cf8!important;color:white!important}.badge.badge-kw-7{background-color:#fb923c!important;color:white!important}.badge.badge-kw-8{background-color:#a78bfa!important;color:white!important}.badge.badge-kw-9{background-color:#f472b6!important;color:white!important}.badge.badge-kw-10{background-color:#5eead4!important;color:#0f766e!important}.badge.badge-kw-11{background-color:#a8a29e!important;color:#292524!important}.badge.badge-kw-12{background-color:#99f6e4!important;color:#0d9488!important}.badge.badge-kw-13{background-color:#bef264!important;color:#365314!important}.badge.badge-kw-14{background-color:#f87171!important;color:white!important}.badge.badge-kw-15{background-color:#38bdf8!important;color:white!important}.badge.badge-kw-16{background-color:#d4d4d8!important;color:#27272a!important}.badge.badge-kw-17{background-color:#e879f9!important;color:#581c87!important}.badge.badge-kw-18{background-color:#a3e635!important;color:#3f6212!important}.badge.badge-kw-19{background-color:#fcd34d!important;color:#78350f!important}.badge.badge-kw-20{background-color:#6366f1!important;color:white!important}.keyword-filter-btn{cursor:pointer;transition:all .2s ease;font-size:.85em;padding:.4em .8em;display:inline-block;margin-bottom:.25rem}.keyword-filter-btn:hover{opacity:.8;transform:translateY(-1px)}.keyword-filter-btn.active{box-shadow:0 2px 4px rgba(0,0,0,0.2);font-weight:bold}.keywords .badge{font-size:.75em;padding:.3em .6em;margin-bottom:.25rem;display:inline-block}.keyword-filters{line-height:1.8;margin-top:1rem!important;margin-bottom:1rem!important}</style> <div class="keyword-filters" style="margin-top: 0.5rem; margin-bottom: 0.5rem;"> <span class="badge badge-light keyword-filter-btn active mr-1" data-filter="all">Show all</span> <span class="badge badge-kw-1 keyword-filter-btn mr-1" data-filter="nerf">NeRF</span> <span class="badge badge-kw-2 keyword-filter-btn mr-1" data-filter="3d representation">3D Representation</span> <span class="badge badge-kw-3 keyword-filter-btn mr-1" data-filter="bayesian learning">Bayesian Learning</span> <span class="badge badge-kw-4 keyword-filter-btn mr-1" data-filter="vision language model">Vision Language Model</span> <span class="badge badge-kw-5 keyword-filter-btn mr-1" data-filter="robot navigation">Robot Navigation</span> <span class="badge badge-kw-6 keyword-filter-btn mr-1" data-filter="3d perception">3D Perception</span> <span class="badge badge-kw-7 keyword-filter-btn mr-1" data-filter="embedded systems">Embedded Systems</span> <span class="badge badge-kw-8 keyword-filter-btn mr-1" data-filter="gaussian process">Gaussian Process</span> <span class="badge badge-kw-9 keyword-filter-btn mr-1" data-filter="scene representation">Scene Representation</span> <span class="badge badge-kw-10 keyword-filter-btn mr-1" data-filter="loop closing">Loop Closing</span> <span class="badge badge-kw-11 keyword-filter-btn mr-1" data-filter="place recognition">Place Recognition</span> <span class="badge badge-kw-13 keyword-filter-btn mr-1" data-filter="slam">SLAM</span> </div> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">RA-L</abbr> <figure> <picture> <img src="/assets/img/publication_preview/lamp.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="lamp.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lee2025lamp" class="col-sm-8"> <div class="title">LAMP: Implicit Language Map for Robot Navigation</div> <div class="author"> <em>Sibaek Lee</em>, Hyeonwoo Yu, Giseop Kim, and Sunwook Choi </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters (RA-L)</em>, 2025 </div> <div class="periodical"> </div> <div class="keywords mt-2"> <span class="badge badge-kw-4 mr-1 keyword-filter" data-keyword="vision language model">Vision Language Model</span> <span class="badge badge-kw-5 mr-1 keyword-filter" data-keyword="robot navigation">Robot Navigation</span> <span class="badge badge-kw-9 mr-1 keyword-filter" data-keyword="scene representation">Scene Representation</span> <span class="badge badge-kw-7 mr-1 keyword-filter" data-keyword="embedded systems">Embedded Systems</span> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://minjae-lulu.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://lab-of-ai-and-robotics.github.io/LAMP/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Recent advances in vision-language models have made zero-shot navigation feasible, enabling robots to interpret and follow natural language instructions without requiring labeling. However, existing methods that explicitly store language vectors in grid or node-based maps struggle to scale to large environments due to excessive memory requirements and limited resolution for fine-grained planning. We introduce LAMP (Language Map), a novel neural language field-based navigation framework that learns a continuous, language-driven map and directly leverages it for fine-grained path generation. Unlike prior approaches, our method encodes language features as an implicit neural field rather than storing them explicitly at every location. By combining this implicit representation with a sparse graph, LAMP supports efficient coarse path planning and then performs gradient-based optimization in the learned field to refine poses near the goal. Our two-stage pipeline of coarse graph search followed by language-driven, gradient-guided optimization is the first application of an implicit language map for precise path generation. This refinement is particularly effective at selecting goal regions not directly observed by leveraging semantic similarities in the learned feature space. To further enhance robustness, we adopt a Bayesian framework that models embedding uncertainty via the von Mises–Fisher distribution, thereby improving generalization to unobserved regions. To scale to large environments, LAMP employs a graph sampling strategy that prioritizes spatial coverage and embedding confidence, retaining only the most informative nodes and substantially reducing computational overhead. Our experimental results, both in NVIDIA Isaac Sim and on a real multi-floor building, demonstrate that LAMP outperforms existing explicit methods in both memory efficiency and fine-grained goal-reaching accuracy, opening new possibilities for scalable, language-driven robot navigation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Preprint</abbr> <figure> <picture> <img src="/assets/img/publication_preview/e3dp.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="e3dp.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lee2025efficient" class="col-sm-8"> <div class="title">Efficient 3D Perception on Embedded Systems via Interpolation-Free Tri-Plane Lifting and Volume Fusion</div> <div class="author"> <em>Sibaek Lee</em>, Jiung Yeon, and Hyeonwoo Yu </div> <div class="periodical"> <em>arXiv preprint arXiv:2509.14641</em>, 2025 </div> <div class="periodical"> </div> <div class="keywords mt-2"> <span class="badge badge-kw-6 mr-1 keyword-filter" data-keyword="3d perception">3D Perception</span> <span class="badge badge-kw-7 mr-1 keyword-filter" data-keyword="embedded systems">Embedded Systems</span> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2509.14641" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/Lab-of-AI-and-Robotics/E3D-Perception" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Dense 3D convolutions provide high accuracy for perception but are too computationally expensive for real-time robotic systems. Existing tri-plane methods rely on 2D image features with interpolation, point-wise queries, and implicit MLPs, which makes them computationally heavy and unsuitable for embedded 3D inference. As an alternative, we propose a novel interpolation-free tri-plane lifting and volumetric fusion framework, that directly projects 3D voxels into plane features and reconstructs a feature volume through broadcast and summation. This shifts nonlinearity to 2D convolutions, reducing complexity while remaining fully parallelizable. To capture global context, we add a low-resolution volumetric branch fused with the lifted features through a lightweight integration layer, yielding a design that is both efficient and end-to-end GPU–accelerated. To validate the effectiveness of the proposed method, we conduct experiments on classification, completion, segmentation, and detection, and we map the trade-off between efficiency and accuracy across tasks. Results show that classification and completion retain or improve accuracy, while segmentation and detection trade modest drops in accuracy for significant computational savings. On-device benchmarks on an NVIDIA Jetson Orin nano confirm robust real-time throughput, demonstrating the suitability of the approach for embedded robotic perception.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">RA-L</abbr> <figure> <picture> <img src="/assets/img/publication_preview/gp_nerf.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="gp_nerf.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kang2025spatial" class="col-sm-8"> <div class="title">Spatial Coordinate Transformation for 3D Neural Implicit Mapping</div> <div class="author"> Kyeongsu Kang, Seongbo Ha, <em>Sibaek Lee</em>, and Hyeonwoo Yu </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters</em>, 2025 </div> <div class="periodical"> </div> <div class="keywords mt-2"> <span class="badge badge-kw-1 mr-1 keyword-filter" data-keyword="nerf">NeRF</span> <span class="badge badge-kw-2 mr-1 keyword-filter" data-keyword="3d representation">3D Representation</span> <span class="badge badge-kw-8 mr-1 keyword-filter" data-keyword="gaussian process">Gaussian Process</span> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/11106682" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Lab-of-AI-and-Robotics/SCT_NIM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Implicit Neural Representation (INR)-based SLAM has a critical issue where all keyframes must be stored in memory for post-training whenever a remapping is needed due to the neural network’s weights themselves representing the map. To address this, previous INR-based SLAM proposed methods to modify INR-based maps without changing the neural network’s weights. However, these approaches suffer from low memory efficiency and increased space complexity. In this paper, we introduce a remapping method for INR-based maps that does not require post-training the neural network’s weights and needed low space cost. The problem of function modification, such as updating a map defined as a neural network function, can be viewed as transforming the function’s domain. Leveraging function domain transformation, we propose a method to update INR-based maps by identifying the transformation function between the post-optimization and pre-optimization domains. Additionally, to prevent cases where the transformation between the post-optimization and pre-optimization domains does not form a one-to-many relationship, we introduce a temporal domain and propose a method to find the spatial coordinate transformation function accordingly. Evaluations in INR-based techniques demonstrate that our proposed method effectively update to maps while requiring significantly less memory compared to existing remapping approaches.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">RA-L</abbr> <figure> <picture> <img src="/assets/img/publication_preview/bayesian.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="bayesian.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lee2025bayesian" class="col-sm-8"> <div class="title">Bayesian NeRF: Quantifying Uncertainty With Volume Density for Neural Implicit Fields</div> <div class="author"> <em>Sibaek Lee</em>, Kyeongsu Kang, Seongbo Ha, and Hyeonwoo Yu </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters</em>, 2025 </div> <div class="periodical"> </div> <div class="keywords mt-2"> <span class="badge badge-kw-1 mr-1 keyword-filter" data-keyword="nerf">NeRF</span> <span class="badge badge-kw-2 mr-1 keyword-filter" data-keyword="3d representation">3D Representation</span> <span class="badge badge-kw-3 mr-1 keyword-filter" data-keyword="bayesian learning">Bayesian Learning</span> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2404.06727" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://ieeexplore.ieee.org/document/10829678" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=wp5jW4S_jqo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/Lab-of-AI-and-Robotics/Bayesian_NeRF" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We present a Bayesian Neural Radiance Field (NeRF), which explicitly quantifies uncertainty in the volume density by modeling uncertainty in the occupancy, without the need for additional networks, making it particularly suited for challenging observations and uncontrolled image environments. NeRF diverges from traditional geometric methods by providing an enriched scene representation, rendering color and density in 3D space from various viewpoints. However, NeRF encounters limitations in addressing uncertainties solely through geometric structure information, leading to inaccuracies when interpreting scenes with insufficient real-world observations. While previous efforts have relied on auxiliary networks, we propose a series of formulation extensions to NeRF that manage uncertainties in density, both color and density, and occupancy, all without the need for additional networks. In experiments, we show that our method significantly enhances performance on RGB and depth images in the comprehensive dataset. Given that uncertainty modeling aligns well with the inherently uncertain environments of Simultaneous Localization and Mapping (SLAM), we applied our approach to SLAM systems and observed notable improvements in mapping and tracking performance. These results confirm the effectiveness of our Bayesian NeRF approach in quantifying uncertainty based on geometric structure, making it a robust solution for challenging real-world scenarios.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IROS</abbr> <figure> <picture> <img src="/assets/img/publication_preview/justflip.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="justflip.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lee2024just" class="col-sm-8"> <div class="title">Just flip: Flipped observation generation and optimization for neural radiance fields to cover unobserved view</div> <div class="author"> <em>Sibaek Lee</em>, Kyeongsu Kang, and Hyeonwoo Yu </div> <div class="periodical"> <em>In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2024 </div> <div class="periodical"> </div> <div class="keywords mt-2"> <span class="badge badge-kw-1 mr-1 keyword-filter" data-keyword="nerf">NeRF</span> <span class="badge badge-kw-2 mr-1 keyword-filter" data-keyword="3d representation">3D Representation</span> <span class="badge badge-kw-3 mr-1 keyword-filter" data-keyword="bayesian learning">Bayesian Learning</span> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2303.06335" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://ieeexplore.ieee.org/abstract/document/10802266" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=ClNg3GSr0jw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/Lab-of-AI-and-Robotics/Just_Flip" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>With the advent of Neural Radiance Field (NeRF), representing 3D scenes through multiple observations has shown significant improvements. Since this cutting-edge technique can obtain high-resolution renderings by interpolating dense 3D environments, various approaches have been proposed to apply NeRF for the spatial understanding of robot perception. However, previous works are challenging to represent unobserved scenes or views on the unexplored robot trajectory, as these works do not take into account 3D reconstruction without observation information. To overcome this problem, we propose a method to generate flipped observation in order to cover absent observation for unexplored robot trajectory. Our approach involves a data augmentation technique for 3D reconstruction using NeRF, by flipping observed images and estimating the 6DOF poses of the flipped cameras. Furthermore, to ensure the NeRF model operates robustly in general scenarios, we also propose a training method that adjusts the flipped pose and considers the uncertainty in flipped images accordingly. Our technique does not utilize an additional network, making it simple and fast, thus ensuring its suitability for robotic applications where real-time performance is crucial.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Preprint</abbr> <figure> <picture> <img src="/assets/img/publication_preview/nfc.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="nfc.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kang2023necessity" class="col-sm-8"> <div class="title">Necessity feature correspondence estimation for large-scale global place recognition and relocalization</div> <div class="author"> Kyeongsu Kang, <em>Minjae Lee</em>, and Hyeonwoo Yu </div> <div class="periodical"> <em>arXiv preprint arXiv:2303.06308</em>, 2023 </div> <div class="periodical"> </div> <div class="keywords mt-2"> <span class="badge badge-kw-10 mr-1 keyword-filter" data-keyword="loop closing">Loop Closing</span> <span class="badge badge-kw-11 mr-1 keyword-filter" data-keyword="place recognition">Place Recognition</span> <span class="badge badge-kw-13 mr-1 keyword-filter" data-keyword="slam">SLAM</span> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2303.06308" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>To find the accurate global 6-DoF transform by feature matching approach, various end-to-end architectures have been proposed. However, existing methods have typically not considered the geometrical false correspondence of features, resulting in unnecessary features being involved in global place recognition and relocalization. In this paper, we introduce a robust correspondence estimation method by removing unnecessary features and highlighting necessary features simultaneously. To emphasize necessary features while disregarding unnecessary ones, we leverage the geometric correlation between two scenes represented in 3D LiDAR point clouds. nWe thus introduce the correspondence auxiliary loss that finds key correlations based on the point align algorithm, achieving end-to-end training of the proposed networks with robust correspondence estimation. Considering the ground with numerous plane patches can disrupt correspondence estimation by acting as an outlier, we propose a preprocessing step aimed at mitigating the influence of dominant plane patches from the perspective of addressing negative correspondences. Evaluation results on the dynamic urban driving dataset show that our proposed method can improve the performances of both global place recognition and relocalization tasks, highlighting the significance of estimating robust feature correspondence in these processes.</p> </div> </div> </div> </li></ol> </div> <script>
document.addEventListener('DOMContentLoaded', function() {
  const filterBtns = document.querySelectorAll('.keyword-filter-btn');
  const publications = document.querySelectorAll('.publications .bibliography > li');

  filterBtns.forEach(btn => {
    btn.addEventListener('click', function() {
      const filter = this.getAttribute('data-filter');

      // Update active button
      filterBtns.forEach(b => b.classList.remove('active'));
      this.classList.add('active');

      // Filter publications
      publications.forEach(pub => {
        if (filter === 'all') {
          pub.style.display = '';
        } else {
          const keywords = pub.querySelectorAll('.keyword-filter');
          let hasKeyword = false;
          keywords.forEach(keyword => {
            if (keyword.getAttribute('data-keyword') === filter) {
              hasKeyword = true;
            }
          });
          pub.style.display = hasKeyword ? '' : 'none';
        }
      });
    });
  });
});
</script> <div class="social" style="margin-top: 2rem;"> <div class="contact-icons"> <a href="mailto:%6C%6D%6A%62%73%6A%31@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/sibaek-lee" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/sibaek-lee-008b1a2a1" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://scholar.google.com/citations?user=BG206AgAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://youtube.com/@aigsmusic" title="YouTube" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> <a href="https://www.youtube.com/@genesisbeats8070" title="Composition" rel="external nofollow noopener" target="_blank"> <img src="https://twemoji.maxcdn.com/v/latest/72x72/1f3b9.png" alt="Composition"> </a> </div> <div class="contact-note">If you are interested in my research, feel free to contact me :) </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Sibaek Lee. Last updated: September 28, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>